{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "yi1b9Svhk-Qy",
    "outputId": "0c158bb4-d1b9-4fdb-b74c-5a8708440e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WN6w7wL-b-eI",
    "outputId": "05597630-7bdd-4b6e-871f-38d8817d7f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/NLP/cnn-text-classification-pytorch\n"
     ]
    }
   ],
   "source": [
    "cd drive/My\\ Drive/NLP/cnn-text-classification-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "1BWsbXx2sziz",
    "outputId": "ab0dbb20-82d6-48ef-9c94-f585428671a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE   mydatasets.py  \u001b[0m\u001b[01;34mrt-polaritydata\u001b[0m/                \u001b[01;34msnapshot\u001b[0m/\n",
      "main.py   \u001b[01;34m__pycache__\u001b[0m/   rt-polaritydata.README.1.0.txt  train.py\n",
      "model.py  README.md      rt-polaritydata.tar             \u001b[01;34mtry1\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33Gc00uhupOJ"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_6KujgZs7Jh"
   },
   "outputs": [],
   "source": [
    "path_to_h5py = \"../sent-conv-torch/custom.hdf5\"\n",
    "hf = h5py.File(path_to_h5py,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueErB-UG3vAP"
   },
   "outputs": [],
   "source": [
    "keys = list(hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv2wViTX34U8"
   },
   "outputs": [],
   "source": [
    "w2v = hf['w2v']\n",
    "data = hf['train']\n",
    "label = hf['train_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJm4dsGr4D1P"
   },
   "outputs": [],
   "source": [
    "w2v = hf.get('w2v').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilGkYDlR4U-s"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = hf.get('train').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JrGQSFTg6iO3"
   },
   "outputs": [],
   "source": [
    "label = hf.get('train_label').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMJIORYt6rcO"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "this = 0\n",
    "for x in data:\n",
    "  c = 0\n",
    "  l = len(x) - 1\n",
    "  while(x[l]==1 and l>=0):\n",
    "    c+=1\n",
    "    l-=1\n",
    "  c = len(x) - c\n",
    "  if c > max_len:\n",
    "    max_len = c\n",
    "    this = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r6rQ0MX59hT4",
    "outputId": "0dbcbcde-be23-4ba9-f8ef-fd028cda5958"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6jUjLBl-3R4"
   },
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "  for j in range(len(data[i])):\n",
    "    data[i][j] = data[i][j] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3Ku-rfHDyDP"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-50Bz2nODYbd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GI1RkrQG7nu2"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UeKQ7CbDefO"
   },
   "outputs": [],
   "source": [
    "ones = []\n",
    "zeros = []\n",
    "for i in range(len(label)):\n",
    "  if label[i]==1:\n",
    "    ones.append(i)\n",
    "  else:\n",
    "    zeros.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wg2KNVC2VBBp"
   },
   "outputs": [],
   "source": [
    "ones = np.array(ones)\n",
    "zeros = np.array(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJYBNw76VI3D"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(ones)\n",
    "np.random.shuffle(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4gOqsUBVIEd"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "split = .8\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "count = 0\n",
    "for i in range(len(ones)):\n",
    "  if count<= split*len(ones)+1:\n",
    "    train_indices.append(ones[i])\n",
    "    count+=1\n",
    "  else:\n",
    "    val_indices.append(ones[i])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(zeros)):\n",
    "  if count<= split*len(zeros) +1:\n",
    "    train_indices.append(zeros[i])\n",
    "    count+=1\n",
    "  else:\n",
    "    val_indices.append(zeros[i])  \n",
    "\n",
    "val_indices = np.array(val_indices)\n",
    "train_indices = np.array(train_indices)\n",
    "np.random.shuffle(val_indices)\n",
    "np.random.shuffle(train_indices)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3kPsQBUW_lBd",
    "outputId": "2e37b81c-5c2b-414e-a8bf-98a67cff8cfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10403"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z9QmVAOU_rPu",
    "outputId": "7dabf41b-b01e-4d7d-e86c-e55ecfd749f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2597"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A97D79lm_sIw"
   },
   "outputs": [],
   "source": [
    "class text_dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        # Begin\n",
    "        # self.train = train  # boolean to check if train or test\n",
    "        # if train:\n",
    "        #     self.root_dir = os.path.join(root_dir , 'trainval')\n",
    "        #     labels_path = os.path.join(self.root_dir, 'labels2.txt')\n",
    "        #     self.names = [(name.split(\" \")[0].strip() , name.split(\" \")[1].strip()) for name in open(labels_path)]\n",
    "        #     self.names = sorted(self.names , key=lambda x: x[1])\n",
    "        # else:\n",
    "        #     self.root_dir = os.path.join(root_dir , 'test/VOCdevkit/VOC2007/')\n",
    "        #     test_path = os.path.join(self.root_dir, 'list.txt')\n",
    "        #     self.names = [name.strip() for name in open(test_path)]\n",
    "        #     self.names = sorted(self.names , key=lambda x: x[1])\n",
    "                    \n",
    "        \n",
    "        # self.transform = transform            # transforms to be done \n",
    "\n",
    "        self.data = data\n",
    "        self.labels = label\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        l = torch.tensor(self.labels[idx]).long()\n",
    "        sentence = torch.tensor(self.data[idx]).long()\n",
    "\n",
    "        return sentence, l\n",
    "\n",
    "        # Begin\n",
    "        # if self.train:\n",
    "        #     name = self.names[idx][0]\n",
    "        #     label = classes.index(self.names[idx][1])\n",
    "        #     image_path = os.path.join(self.root_dir , 'croped2/' + name)  # load image\n",
    "        #     img = image_loader(image_path)\n",
    "            \n",
    "        #     if self.transform is not None:\n",
    "        #         img = self.transform(img)  # apply transforms\n",
    "            \n",
    "        #     return img, label   # return image's tensor and label\n",
    "\n",
    "        \n",
    "    # def test_iter(self, idx):    # only for Testing.\n",
    "    #     '''This function loads test image with index = idx, and return's  it's path and the list of bounding boxes:\n",
    "    #     [class, x1,y1,x2,y2]'''\n",
    "        \n",
    "    #     image_name = self.names[idx]\n",
    "    #     annot_path = os.path.join(self.root_dir , 'Annotations')\n",
    "    #     image_path = os.path.join(self.root_dir , 'Images/' + image_name)\n",
    "    #     img = image_path\n",
    "\n",
    "    #     tree = ET.parse(os.path.join(annot_path , image_name.split('.')[0] + '.xml'))\n",
    "    #     root = tree.getroot()\n",
    "    #     id =0\n",
    "    #     bounding_boxes=[]\n",
    "    #     for child in root.iter('object'):\n",
    "    #         label = []\n",
    "    #         name = child.find('name').text\n",
    "    #         if name != 'aeroplane' and name != 'chair' and name != 'bottle':\n",
    "    #             continue\n",
    "    #         label.append(classes.index(name))\n",
    "    #         bndbox = child.find('bndbox')   # load bounding box\n",
    "    #         label.append(int(bndbox.find('xmin').text))\n",
    "    #         label.append(int(bndbox.find('ymin').text))\n",
    "    #         label.append(int(bndbox.find('xmax').text))\n",
    "    #         label.append(int(bndbox.find('ymax').text))\n",
    "    #         bounding_boxes.append(np.array(label))\n",
    "\n",
    "    #     return img, np.array(bounding_boxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoMSOGKqDvps"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):        \n",
    "\n",
    "    def __init__(self, args, weight_matrix):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        self.embed, num_embeddings, embedding_dim = self.create_emb_layer(weight_matrix, args.non_static)\n",
    "\n",
    "        V = num_embeddings\n",
    "        D = embedding_dim\n",
    "        C = args.class_num\n",
    "        Ci = 1\n",
    "        Co = args.kernel_num\n",
    "        Ks = args.kernel_sizes\n",
    "\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "    \n",
    "    def create_emb_layer(self, weight_matrix, train_weights=False):\n",
    "        num_embeddings, embedding_dim = weight_matrix.size()\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "        if train_weights:\n",
    "            emb_layer.weight.requires_grad = True\n",
    "        else:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "\n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "        # print(x.size())\n",
    "        \n",
    "        if self.args.non_static == False:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-6-nHIRELWr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "def train(train_iter, dev_iter, model, args):\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        print(\"On Cuda\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    weights = [1,10]\n",
    "    weights = torch.FloatTensor(weights).cuda()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    to_return = {}\n",
    "    max_so_far = 0.0\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        model.train()\n",
    "        train_preds = None\n",
    "        train_labels = None\n",
    "        running_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        mini_batch = 0\n",
    "        print(\"epoch: \",epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        for feature, labels in train_iter:\n",
    "            mini_batch+=1\n",
    "            \n",
    "            if args.cuda:\n",
    "                feature, labels = feature.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(feature)\n",
    "\n",
    "            # print('outputs size', outputs.size())\n",
    "            # print('target vector', target.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            steps += 1\n",
    "            # if steps % args.log_interval == 0:\n",
    "                # corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "                # accuracy = 100.0 * corrects/args.batch_size\n",
    "                # sys.stdout.write(\n",
    "                #     '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n",
    "                #                                                              loss.data, \n",
    "                #                                                              accuracy,\n",
    "                #                                                              corrects,\n",
    "                #                                                              args.batch_size))\n",
    "            if train_preds is None or train_labels is None:\n",
    "              train_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
    "              train_labels = labels.cpu().numpy().flatten()\n",
    "            else:\n",
    "              train_preds = np.concatenate((train_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
    "              train_labels = np.concatenate((train_labels, labels.cpu().numpy().flatten()))\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            if mini_batch % 100 == 0:    # print every 100 mini-batches\n",
    "              print('[%d, %5d] loss: %.3f' % (epoch , mini_batch , running_loss / 100))\n",
    "              running_loss = 0.0\n",
    "            \n",
    "            # if steps % args.test_interval == 0:\n",
    "            #     dev_acc = eval(dev_iter, model, args)\n",
    "            #     if dev_acc > best_acc:\n",
    "            #         best_acc = dev_acc\n",
    "            #         last_step = steps\n",
    "            #         if args.save_best:\n",
    "            #             save(model, args.save_dir, 'best', steps)\n",
    "            #     else:\n",
    "            #         if steps - last_step >= args.early_stop:\n",
    "            #             print('early stop by {} steps.'.format(args.early_stop))\n",
    "            # elif steps % args.save_interval == 0:\n",
    "            #     save(model, args.save_dir, 'snapshot', steps)\n",
    "        \n",
    "        print(\"Training loss in epoch %d is %.3f\" % (epoch , total_loss / len(train_loader)))\n",
    "        print(\"Training accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(train_labels, train_preds) * 100))\n",
    "        print(\"Training precision in epoch %d is %.3f\" % (epoch , precision_score(train_labels, train_preds) * 100))\n",
    "        print(\"Training recall in epoch %d is %.3f\" % (epoch , recall_score(train_labels, train_preds) * 100))\n",
    "        print(\"Training F1-score in epoch %d is %.3f\" % (epoch, f1_score(train_labels, train_preds) * 100))\n",
    "        \n",
    "        eval(dev_iter, model, criterion, epoch, args, to_return, max_so_far)\n",
    "        print()\n",
    "        print(\"-\"*90)\n",
    "        print()\n",
    "        \n",
    "    return to_return\n",
    "\n",
    "\n",
    "def eval(data_iter, model, criterion, epoch, args, to_return, max_so_far):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_preds = None\n",
    "    test_labels = None\n",
    "    test_inputs = None\n",
    "    # corrects, avg_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in data_iter:\n",
    "            inputs, labels = data\n",
    "            if args.cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "          \n",
    "            test_loss += loss.item()\n",
    "            if test_preds is None or test_labels is None:\n",
    "                test_preds = np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()\n",
    "                test_labels = labels.cpu().numpy().flatten()\n",
    "                test_inputs = inputs.cpu().numpy()\n",
    "            else:\n",
    "                test_preds = np.concatenate((test_preds, np.argmax(outputs.detach().cpu().numpy(), axis=1).flatten()))\n",
    "                test_labels = np.concatenate((test_labels, labels.cpu().numpy().flatten()))\n",
    "                test_inputs = np.concatenate((test_inputs, inputs.cpu().numpy()), axis = 0)\n",
    "\n",
    "            # test_accuracy += flat_accuracy(outputs[0], labels)\n",
    "    \n",
    "    print(\"Test loss in epoch %d is %.3f\" % (epoch, test_loss / len(val_loader)))\n",
    "    print(\"Test accuracy in epoch %d is %.3f\" % (epoch, accuracy_score(test_labels, test_preds) * 100))\n",
    "    print(\"Test precision in epoch %d is %.3f\" % (epoch, precision_score(test_labels, test_preds) * 100))\n",
    "    print(\"Test recall in epoch %d is %.3f\" % (epoch, recall_score(test_labels, test_preds) * 100))\n",
    "    print(\"Test F1-score in epoch %d is %.3f\" % (epoch, f1_score(test_labels, test_preds) * 100))\n",
    "    if f1_score(test_labels, test_preds)*100 > max_so_far:\n",
    "      max_so_far = f1_score(test_labels, test_preds)*100\n",
    "      to_return['pred'] = test_preds\n",
    "      to_return['label'] = test_labels\n",
    "      to_return['inputs'] = test_inputs\n",
    "\n",
    "    # for feature, target in data_iter:\n",
    "    #     # feature, target = batch.text, batch.label\n",
    "    #     # feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
    "    #     if args.cuda:\n",
    "    #         feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "    #     logit = model(feature)\n",
    "    #     loss = criterion(logit, target)\n",
    "\n",
    "    #     avg_loss += loss.data\n",
    "    #     corrects += (torch.max(logit, 1)\n",
    "    #                  [1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "    # size = len(data_iter.dataset)*0.2\n",
    "    # avg_loss /= size\n",
    "    # accuracy = 100.0 * corrects/size\n",
    "    # print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "    #                                                                    accuracy, \n",
    "    #                                                                    corrects, \n",
    "    #                                                                    size))\n",
    "\n",
    "\n",
    "    # return accuracy\n",
    "\n",
    "    # print(\"Val loss in epoch %d is %.3f\" % (epoch + 1, test_loss / len(test_loader)))\n",
    "    # print(\"Val accuracy in epoch %d is %.3f\" % (epoch + 1, accuracy_score(test_labels, test_preds) * 100))\n",
    "    # print(\"Val precision in epoch %d is %.3f\" % (epoch + 1, precision_score(test_labels, test_preds) * 100))\n",
    "    # print(\"Val recall in epoch %d is %.3f\" % (epoch + 1, recall_score(test_labels, test_preds) * 100))\n",
    "    # print(\"Val   F1-score in epoch %d is %.3f\" % (epoch + 1, f1_score(test_labels, test_preds) * 100))\n",
    "\n",
    "\n",
    "def predict(text, model, text_field, label_feild, cuda_flag):\n",
    "    assert isinstance(text, str)\n",
    "    model.eval()\n",
    "    # text = text_field.tokenize(text)\n",
    "    text = text_field.preprocess(text)\n",
    "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
    "    x = torch.tensor(text)\n",
    "    x = autograd.Variable(x)\n",
    "    if cuda_flag:\n",
    "        x = x.cuda()\n",
    "    print(x)\n",
    "    output = model(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
    "    return label_feild.vocab.itos[predicted.data[0]+1]\n",
    "\n",
    "\n",
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "oDSNmGqiEanf",
    "outputId": "d7fb3dd6-ffc5-4573-ec25-e1381aa67791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Parameters:\n",
      "\tBATCH_SIZE=32\n",
      "\tCLASS_NUM=2\n",
      "\tCUDA=True\n",
      "\tDEVICE=-1\n",
      "\tDROPOUT=0.5\n",
      "\tEARLY_STOP=1000\n",
      "\tEMBED_DIM  =300\n",
      "\tEMBED_NUM=300\n",
      "\tEPOCHS=15\n",
      "\tKERNEL_NUM=100\n",
      "\tKERNEL_SIZES=[3, 4, 5]\n",
      "\tLOG_INTERVAL=1\n",
      "\tLR=0.001\n",
      "\tMAX_NORM=3.0\n",
      "\tNON_STATIC=False\n",
      "\tPREDICT=None\n",
      "\tSAVE_BEST=True\n",
      "\tSAVE_DIR=try1/2020-02-01_16-12-10\n",
      "\tSAVE_INTERVAL=500\n",
      "\tSHUFFLE=True\n",
      "\tSNAPSHOT=None\n",
      "\tTEST=False\n",
      "\tTEST_INTERVAL=100\n",
      "Training sentences:  10400\n",
      "Validation sentences:  2599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "# import torchtext.data as data\n",
    "# import torchtext.datasets as datasets\n",
    "# import model\n",
    "# import train\n",
    "# import mydatasets\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='CNN text classificer')\n",
    "# learning\n",
    "parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n",
    "parser.add_argument('-epochs', type=int, default=15, help='number of epochs for train [default: 256]')\n",
    "parser.add_argument('-batch-size', type=int, default=32, help='batch size for training [default: 64]')\n",
    "parser.add_argument('-log-interval',  type=int, default=1,   help='how many steps to wait before logging training status [default: 1]')\n",
    "parser.add_argument('-test-interval', type=int, default=100, help='how many steps to wait before testing [default: 100]')\n",
    "parser.add_argument('-save-interval', type=int, default=500, help='how many steps to wait before saving [default:500]')\n",
    "parser.add_argument('-save-dir', type=str, default='try1', help='where to save the snapshot')\n",
    "parser.add_argument('-early-stop', type=int, default=1000, help='iteration numbers to stop without performance increasing')\n",
    "parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n",
    "# data \n",
    "parser.add_argument('-shuffle', action='store_true', default=True, help='shuffle the data every epoch')\n",
    "# model\n",
    "parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n",
    "parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n",
    "parser.add_argument('-embed-dim''''  ''', type=int, default=300, help='number of embedding dimension [default: 128]')\n",
    "parser.add_argument('-kernel-num', type=int, default=100, help='number of each kind of kernel')\n",
    "parser.add_argument('-kernel-sizes', type=str, default='3,4,5', help='comma-separated kernel size to use for convolution')\n",
    "parser.add_argument('-non_static', action='store_true', default=False, help='fix the embedding')\n",
    "# device\n",
    "parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n",
    "parser.add_argument('-no-cuda', action='store_true', default=False, help='disable the gpu')\n",
    "# option\n",
    "parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n",
    "parser.add_argument('-predict', type=str, default=None, help='predict the sentence given')\n",
    "parser.add_argument('-test', action='store_true', default=False, help='train or test')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "# load data \n",
    "print(\"\\nLoading data...\")\n",
    "# text_field = data.Field(lower=True)\n",
    "# label_field = data.Field(sequential=False)\n",
    "# train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n",
    "dataset = text_dataset(data, label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n",
    "                                           sampler = train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch_size,\n",
    "                                           sampler = valid_sampler)\n",
    "\n",
    "# train_iter, dev_iter, test_iter = sst(text_field, label_field, device=-1, repeat=False)\n",
    "\n",
    "\n",
    "# update args and print\n",
    "args.embed_num = weight_matrix.size()[1]\n",
    "args.class_num = 2 #binary\n",
    "args.cuda = (not args.no_cuda) and torch.cuda.is_available(); del args.no_cuda\n",
    "args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n",
    "args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "    print(\"\\t{}={}\".format(attr.upper(), value))\n",
    "print(\"Training sentences: \", int(len(dataset)*split))\n",
    "print(\"Validation sentences: \", int(len(dataset)*(1-split)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04O8R9EVJSDJ"
   },
   "outputs": [],
   "source": [
    "# model \n",
    "cnn = CNN_Text(args, weight_matrix)\n",
    "if args.snapshot is not None:\n",
    "    print('\\nLoading model from {}...'.format(args.snapshot))\n",
    "    cnn.load_state_dict(torch.load(args.snapshot))\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.device)\n",
    "    cnn = cnn.cuda()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jSvXvKkJROCO",
    "outputId": "404c5c14-eada-434d-97fd-ef27a6c8bada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Cuda\n",
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.606\n",
      "[1,   200] loss: 0.458\n",
      "[1,   300] loss: 0.471\n",
      "Training loss in epoch 1 is 0.503\n",
      "Training accuracy in epoch 1 is 73.267\n",
      "Training precision in epoch 1 is 26.151\n",
      "Training recall in epoch 1 is 76.052\n",
      "Training F1-score in epoch 1 is 38.919\n",
      "Test loss in epoch 1 is 0.418\n",
      "Test accuracy in epoch 1 is 86.292\n",
      "Test precision in epoch 1 is 43.366\n",
      "Test recall in epoch 1 is 75.779\n",
      "Test F1-score in epoch 1 is 55.164\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  2\n",
      "[2,   100] loss: 0.380\n",
      "[2,   200] loss: 0.357\n",
      "[2,   300] loss: 0.337\n",
      "Training loss in epoch 2 is 0.354\n",
      "Training accuracy in epoch 2 is 84.774\n",
      "Training precision in epoch 2 is 41.389\n",
      "Training recall in epoch 2 is 86.438\n",
      "Training F1-score in epoch 2 is 55.976\n",
      "Test loss in epoch 2 is 0.359\n",
      "Test accuracy in epoch 2 is 84.521\n",
      "Test precision in epoch 2 is 40.722\n",
      "Test recall in epoch 2 is 85.813\n",
      "Test F1-score in epoch 2 is 55.234\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  3\n",
      "[3,   100] loss: 0.271\n",
      "[3,   200] loss: 0.248\n",
      "[3,   300] loss: 0.278\n",
      "Training loss in epoch 3 is 0.266\n",
      "Training accuracy in epoch 3 is 88.138\n",
      "Training precision in epoch 3 is 48.415\n",
      "Training recall in epoch 3 is 90.472\n",
      "Training F1-score in epoch 3 is 63.076\n",
      "Test loss in epoch 3 is 0.374\n",
      "Test accuracy in epoch 3 is 89.488\n",
      "Test precision in epoch 3 is 51.860\n",
      "Test recall in epoch 3 is 77.163\n",
      "Test F1-score in epoch 3 is 62.031\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  4\n",
      "[4,   100] loss: 0.202\n",
      "[4,   200] loss: 0.222\n",
      "[4,   300] loss: 0.192\n",
      "Training loss in epoch 4 is 0.201\n",
      "Training accuracy in epoch 4 is 91.474\n",
      "Training precision in epoch 4 is 57.240\n",
      "Training recall in epoch 4 is 94.335\n",
      "Training F1-score in epoch 4 is 71.248\n",
      "Test loss in epoch 4 is 0.376\n",
      "Test accuracy in epoch 4 is 89.911\n",
      "Test precision in epoch 4 is 53.207\n",
      "Test recall in epoch 4 is 77.509\n",
      "Test F1-score in epoch 4 is 63.099\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  5\n",
      "[5,   100] loss: 0.148\n",
      "[5,   200] loss: 0.164\n",
      "[5,   300] loss: 0.179\n",
      "Training loss in epoch 5 is 0.165\n",
      "Training accuracy in epoch 5 is 93.117\n",
      "Training precision in epoch 5 is 62.705\n",
      "Training recall in epoch 5 is 95.107\n",
      "Training F1-score in epoch 5 is 75.580\n",
      "Test loss in epoch 5 is 0.482\n",
      "Test accuracy in epoch 5 is 92.722\n",
      "Test precision in epoch 5 is 67.241\n",
      "Test recall in epoch 5 is 67.474\n",
      "Test F1-score in epoch 5 is 67.358\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  6\n",
      "[6,   100] loss: 0.126\n",
      "[6,   200] loss: 0.141\n",
      "[6,   300] loss: 0.132\n",
      "Training loss in epoch 6 is 0.131\n",
      "Training accuracy in epoch 6 is 94.425\n",
      "Training precision in epoch 6 is 67.484\n",
      "Training recall in epoch 6 is 96.910\n",
      "Training F1-score in epoch 6 is 79.563\n",
      "Test loss in epoch 6 is 0.406\n",
      "Test accuracy in epoch 6 is 90.412\n",
      "Test precision in epoch 6 is 54.975\n",
      "Test recall in epoch 6 is 76.471\n",
      "Test F1-score in epoch 6 is 63.965\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  7\n",
      "[7,   100] loss: 0.088\n",
      "[7,   200] loss: 0.083\n",
      "[7,   300] loss: 0.086\n",
      "Training loss in epoch 7 is 0.088\n",
      "Training accuracy in epoch 7 is 96.780\n",
      "Training precision in epoch 7 is 78.117\n",
      "Training recall in epoch 7 is 98.970\n",
      "Training F1-score in epoch 7 is 87.315\n",
      "Test loss in epoch 7 is 0.429\n",
      "Test accuracy in epoch 7 is 91.336\n",
      "Test precision in epoch 7 is 58.889\n",
      "Test recall in epoch 7 is 73.356\n",
      "Test F1-score in epoch 7 is 65.331\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  8\n",
      "[8,   100] loss: 0.086\n",
      "[8,   200] loss: 0.079\n",
      "[8,   300] loss: 0.077\n",
      "Training loss in epoch 8 is 0.082\n",
      "Training accuracy in epoch 8 is 96.905\n",
      "Training precision in epoch 8 is 78.969\n",
      "Training recall in epoch 8 is 98.627\n",
      "Training F1-score in epoch 8 is 87.710\n",
      "Test loss in epoch 8 is 0.423\n",
      "Test accuracy in epoch 8 is 90.951\n",
      "Test precision in epoch 8 is 57.181\n",
      "Test recall in epoch 8 is 74.394\n",
      "Test F1-score in epoch 8 is 64.662\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  9\n",
      "[9,   100] loss: 0.074\n",
      "[9,   200] loss: 0.067\n",
      "[9,   300] loss: 0.071\n",
      "Training loss in epoch 9 is 0.070\n",
      "Training accuracy in epoch 9 is 97.260\n",
      "Training precision in epoch 9 is 80.769\n",
      "Training recall in epoch 9 is 99.142\n",
      "Training F1-score in epoch 9 is 89.017\n",
      "Test loss in epoch 9 is 0.475\n",
      "Test accuracy in epoch 9 is 92.491\n",
      "Test precision in epoch 9 is 64.968\n",
      "Test recall in epoch 9 is 70.588\n",
      "Test F1-score in epoch 9 is 67.662\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  10\n",
      "[10,   100] loss: 0.064\n",
      "[10,   200] loss: 0.073\n",
      "[10,   300] loss: 0.075\n",
      "Training loss in epoch 10 is 0.070\n",
      "Training accuracy in epoch 10 is 97.366\n",
      "Training precision in epoch 10 is 81.351\n",
      "Training recall in epoch 10 is 99.227\n",
      "Training F1-score in epoch 10 is 89.404\n",
      "Test loss in epoch 10 is 0.463\n",
      "Test accuracy in epoch 10 is 92.414\n",
      "Test precision in epoch 10 is 64.650\n",
      "Test recall in epoch 10 is 70.242\n",
      "Test F1-score in epoch 10 is 67.330\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  11\n",
      "[11,   100] loss: 0.061\n",
      "[11,   200] loss: 0.063\n",
      "[11,   300] loss: 0.065\n",
      "Training loss in epoch 11 is 0.063\n",
      "Training accuracy in epoch 11 is 97.712\n",
      "Training precision in epoch 11 is 83.514\n",
      "Training recall in epoch 11 is 99.142\n",
      "Training F1-score in epoch 11 is 90.659\n",
      "Test loss in epoch 11 is 0.483\n",
      "Test accuracy in epoch 11 is 92.530\n",
      "Test precision in epoch 11 is 65.273\n",
      "Test recall in epoch 11 is 70.242\n",
      "Test F1-score in epoch 11 is 67.667\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  12\n",
      "[12,   100] loss: 0.065\n",
      "[12,   200] loss: 0.056\n",
      "[12,   300] loss: 0.055\n",
      "Training loss in epoch 12 is 0.058\n",
      "Training accuracy in epoch 12 is 97.943\n",
      "Training precision in epoch 12 is 85.092\n",
      "Training recall in epoch 12 is 98.970\n",
      "Training F1-score in epoch 12 is 91.508\n",
      "Test loss in epoch 12 is 0.476\n",
      "Test accuracy in epoch 12 is 92.068\n",
      "Test precision in epoch 12 is 62.462\n",
      "Test recall in epoch 12 is 71.972\n",
      "Test F1-score in epoch 12 is 66.881\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  13\n",
      "[13,   100] loss: 0.058\n",
      "[13,   200] loss: 0.057\n",
      "[13,   300] loss: 0.054\n",
      "Training loss in epoch 13 is 0.057\n",
      "Training accuracy in epoch 13 is 97.895\n",
      "Training precision in epoch 13 is 84.375\n",
      "Training recall in epoch 13 is 99.657\n",
      "Training F1-score in epoch 13 is 91.381\n",
      "Test loss in epoch 13 is 0.521\n",
      "Test accuracy in epoch 13 is 92.761\n",
      "Test precision in epoch 13 is 66.450\n",
      "Test recall in epoch 13 is 70.588\n",
      "Test F1-score in epoch 13 is 68.456\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  14\n",
      "[14,   100] loss: 0.057\n",
      "[14,   200] loss: 0.054\n",
      "[14,   300] loss: 0.048\n",
      "Training loss in epoch 14 is 0.052\n",
      "Training accuracy in epoch 14 is 98.404\n",
      "Training precision in epoch 14 is 87.927\n",
      "Training recall in epoch 14 is 99.399\n",
      "Training F1-score in epoch 14 is 93.312\n",
      "Test loss in epoch 14 is 0.484\n",
      "Test accuracy in epoch 14 is 92.414\n",
      "Test precision in epoch 14 is 64.465\n",
      "Test recall in epoch 14 is 70.934\n",
      "Test F1-score in epoch 14 is 67.545\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "epoch:  15\n",
      "[15,   100] loss: 0.049\n",
      "[15,   200] loss: 0.052\n",
      "[15,   300] loss: 0.043\n",
      "Training loss in epoch 15 is 0.049\n",
      "Training accuracy in epoch 15 is 98.270\n",
      "Training precision in epoch 15 is 86.947\n",
      "Training recall in epoch 15 is 99.485\n",
      "Training F1-score in epoch 15 is 92.794\n",
      "Test loss in epoch 15 is 0.482\n",
      "Test accuracy in epoch 15 is 92.607\n",
      "Test precision in epoch 15 is 65.595\n",
      "Test recall in epoch 15 is 70.588\n",
      "Test F1-score in epoch 15 is 68.000\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "# train or predict\n",
    "# static\n",
    "if args.predict is not None:\n",
    "    label = predict(args.predict, cnn, text_field, label_field, args.cuda)\n",
    "    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n",
    "elif args.test:\n",
    "    try:\n",
    "        eval(test_iter, cnn, args) \n",
    "    except Exception as e:\n",
    "        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n",
    "else:\n",
    "    print()\n",
    "    try:\n",
    "        val = train(train_loader, val_loader, cnn, args)\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n' + '-' * 89)\n",
    "        print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AMm2zTCiXq_"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwNP0W7jJqmE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8b0a6f408260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_set.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "with open('val_set.pickle', 'wb') as f:\n",
    "  pickle.dump(val, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_glove.pickle','rb') as f:\n",
    "    val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "with open('../sent-conv-torch/custom_word_mapping.txt','r') as f:\n",
    "    for line in f:\n",
    "        [word, idx] = line.strip().split(' ')\n",
    "        mapping[int(idx)-1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "mapping = {}\n",
    "with open(\"custom_mapping.pickle\",'rb') as f:\n",
    "    map = pickle.load(f)\n",
    "    for x in map:\n",
    "        mapping[map[x]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_1.ipynb            look_wrong_preds.md  mydatasets.py  val_set.pickle\n",
      "custom_mapping.pickle  \u001b[0m\u001b[01;32mmain.py\u001b[0m*             README.md\n",
      "LICENSE                model.py             train.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = []\n",
    "false_neg = []\n",
    "for i in range(val['pred'].shape[0]):\n",
    "    if val['pred'][i] != val['label'][i]:\n",
    "        line = \"\"\n",
    "        for x in val['inputs'][i]:\n",
    "            if x!=0:\n",
    "                line = line + mapping[x] + \" \"\n",
    "        if val['pred'][i]:\n",
    "            false_pos.append(line)\n",
    "        else:\n",
    "            false_neg.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives, i.e., wrongly predicted as counterfactuals\n",
      "\n",
      "then , in a routine that would be comically unbelievable were it not a mirror image of the way giuliani has addressed a whole series of issues , he first claims that `` you ca n't really question why the president would say something . '' then says it 's all fine   \n",
      "\n",
      "rather than oppose this , many republicans reasoned that if anyone was going to create a new social programme , it might as well be them .  \n",
      "\n",
      "when asked how he could end them as president if they are led by states , trump said that the standards had been taken over by bureaucrats in washington .  \n",
      "\n",
      "in the past , marketmakers at big banks might have stepped in to limit price moves .  \n",
      "\n",
      "that the doctors initially thought she might have malaria but now they were n't sure what she had .  \n",
      "\n",
      "i just feel too screwed up and all the good years i should have with my family are gone and we ca n't go back god i love them .  \n",
      "\n",
      "by 2016 , 24 states would have passed laws mandating such reviews , partly inspired by brown 's story .  \n",
      "\n",
      "there could be some weather factor there , i 'm not sure , but the numbers have n't been trending quite the way some people would have expected .  \n",
      "\n",
      "it would have been odd to think of storefront businesses where people would fill out your tax returns for you .  \n",
      "\n",
      "but you would n't have known that at his first big 2020 rally in brooklyn , new york , on saturday , which took place not far from where sanders grew up .  \n",
      "\n",
      "if it were struck down , what would the ruling mean for same sex military couples ? since benefits make up about 70 percent of military compensation , invalidating doma could have major effects on the finances of lgbt soldiers .  \n",
      "\n",
      "`` the sense was he was n't prepared for things he knew he should have been prepared for , he did n't have the answers to things he should have had the answers for , '' said one former us government official .  \n",
      "\n",
      "even if all the executions scheduled for this year are carried outwhich is unlikelya total of 33 would be the lowest since 1994 , and would have fallen by two thirds from the peak of 98 in 1999 ( see chart ) .  \n",
      "\n",
      "however , i would have thought that conscientiousness in particular would be associated with a preference for lawful rather than chaotic characters , but they were unrelated .  \n",
      "\n",
      "if it took a richard nixon to go to china , maybe trump could have restored environmental sanity to the republicans .  \n",
      "\n",
      "it added that the company 's acts `` could have been interpreted badly , '' and acknowledged that in certain situations it had identified a link between the working environment and suicide .  \n",
      "\n",
      "but it should have been clear that rules allowing retaliation against alleged currency manipulators , which would almost certainly fall foul of world trade organisation law , were a non starter .  \n",
      "\n",
      "environmental groups had pushed for an earlier iteration of the bill , which would have required the epa .  \n",
      "\n",
      "its headline conclusion is that uk spending on healthcare will have to rise by an average of 3.3 per cent a year over the next 15 years just to maintain nhs provision at current levels , and by at least 4 per cent a year if services are to be improved .  \n",
      "\n",
      "children would be separated from their parents if the families had been apprehended entering the country illegally , john f .  \n",
      "\n",
      "`` if president barack obama had the legal authority to use his discretion to create daca in the first place '' , mr feldman reasons , `` mr trump must have the legal authority to reverse daca on the ground that he considers it to have exceeded mr obama 's powers . '' it is `` cute '' for judge alsup to cite a tweet from mr trump frowning on `` throw [ ing ] out good , educated and accomplished young people who have jobs , some serving in the military '' to show that the president `` publicly favours the very programme the agency has ended ''  but this is neither here nor there , mr feldman writes .  \n",
      "\n",
      "as promised , gov . mark dayton vetoed the legislature 's measure that would have conformed minnesota 's tax code with the new federal changes , because lawmakers would n't go along with his school funding plan  \n",
      "\n",
      "but , if certified , the results would make mr. golden the winner of a four candidate race in which mr. poliquin received 2,632 more first choice votes but failed to reach the 50 percent needed to win  \n",
      "\n",
      "then he said i was never told i had untreated major deppression or my unwanted celibacy twelve years was a crisis that would have normal outcome of murder and or suicide i am not insane at time twenty years clean and sober no history of hallucinating or hearing , seeing things or false memories .  \n",
      "\n",
      "i would never want us to join the euro , but if we were offered british terms , what we have now   \n",
      "\n",
      "as security director , mr. wolfe would have been responsible for ensuring that those rules were upheld .  \n",
      "\n",
      "a few years ago , my parents decided to move from the ranch home they had lived in for 20 years in florida to a smaller home in a senior community nearby where they would have fewer responsibilities and enjoy the company of their peers .  \n",
      "\n",
      "legal experts say any deal taking blackberry private would work best if it had canadian involvement .  \n",
      "\n",
      "if comcast and time warner are allowed to merge , the resulting behemoth would have immense scale .  \n",
      "\n",
      "using simple math , you 'd think that if you had worked 33 years and chose to work one more year , then you 'd boost your benefits by about 1 33 , or 3 % .  \n",
      "\n",
      "sowhile paying the tax in 2010 probably would n't put many smaller plansout of business , it would create some capital issues that would have tobe rectified through higher premium rates in the ensuing years in orderto build the capital base back up , which would likely result in furthermarket share gains by the larger plans in the market , resulting in lesscompetition , a direct contradiction to one of the goals of thelegislation .  \n",
      "\n",
      "he added all capital and leverage ratios were well within targets and ing would have 3.9 billion euros of spare cash after completing its 5 billion euros share buyback and the payment of last year 's dividend .  \n",
      "\n",
      "that he would consider re entering the agreement only if it were `` substantially better '' than the deal offered to president barack obama .  \n",
      "\n",
      "but if we had to choose some highlights , we 'd opt for these unforgettable events and findings .  \n",
      "\n",
      "lugar also refused to say whether he 'd back mourdock if mourdock wins the primary though mourdock had no trouble saying he 'd support lugar .  \n",
      "\n",
      "countrywide had argued that the laws negated obligations it might have had to buy back modified loans .  \n",
      "\n",
      "but he quickly went in and took the huge benefit of the major support operations and made the current customer base of the acquired companies very comfortable with the fact that they would be serviced , they would n't be cut off , they would n't be stranded , and they would have clear paths to upgrading to future products when and if they desired .  \n",
      "\n",
      "he would have felt he had secured his legacy after a triumph last year , when he persuaded shareholders to agree to a paid in capital increase of $ 13bn , expanding the banks lending capacity from $ 60bn to $ 100bn by 2030 .  \n",
      "\n",
      "it 's impossible to tell from the cms report if these goals were accomplished as quickly as they should have been .  \n",
      "\n",
      "i 'll be marching because i must by vl baker : `` i had n't planned to head to nyc this coming weekend for the people 's climate march because i did n't think i would have enough time to be back at work on monday without flying .  \n",
      "\n",
      "about nintendo 's game console , which launched on november 19 , storch said the store had already sold tens of thousands and would have tens of thousands more available soon .  \n",
      "\n",
      "after that adventure , i went straight to my bedroom and collapsed on the bed only to realize that the load of laundry would have finished the drying cycle and if i did n't fold the clothes right now , they 'd turn into a wrinkled mess .  \n",
      "\n",
      "if a filed disclosure were misstated , investors would have legal recourse .  \n",
      "\n",
      "i thought that if i was just doing what the doctor said , i 'd be fine .  \n",
      "\n",
      "if fighting broke out again in kosovo , its serbs would surely be among the first victims .  \n",
      "\n",
      "many of the horrified onlookers would have known that the triangle had been part of a 20,000 strong citywide garment industry strike the previous year .  \n",
      "\n",
      "well , as chance would have it , i had financial difficulty , and , as a result , decided i could n't go on the trip .  \n",
      "\n",
      "if you were asked a basic question , over the last 20 years , has the number of paid malpractice claims in america doubled or been cut in half ? if you listened to most people , you 'd say they must have doubled .  \n",
      "\n",
      "i am now 40 and do n't wish i had known anything more than i did at the time .  \n",
      "\n",
      "`` we never would have expected two years or more out that patients could recover from a stroke , '' said steinberg .  \n",
      "\n",
      "if we were to create a hierarchy of implausibility , homeopathy would certainly be near the top .  \n",
      "\n",
      "by 76 weeks , the gap had narrowed : 39 % of patients on high dose benlysta responded vs. 32 % of those on placebo , a difference that could have been due to chance  \n",
      "\n",
      "i was probably about seven years old and the idea that i 'd picked up a girls ' costume for a seven year boy ; this was about the biggest mistake i think i could have made , and so i began to whine .  \n",
      "\n",
      "given the nervousness across the regionrussia opposes independence but much of the west supports itone would have hoped that nato could also pour some soothing balm .  \n",
      "\n",
      "going back to the book comparison , it would be like if you had to read everything you had already read that session over again , just to get back to the part you were already at .  \n",
      "\n",
      "if these cracks were present on the inner surface of the column when the crane was recertified following the collision and during subsequent annual inspections , they would have been difficult to detect given the limitations of the magnetic particle inspection technique used for crane inspection and the location of the cracks .  \n",
      "\n",
      "`` most investors would have been happy if they got it at 10 percent growth , '' said moran .  \n",
      "\n",
      "she was tried as an adult and the prosecution argued that since she had stolen items following the killing , it could not have been an act of self defense .  \n",
      "\n",
      "`` right now , if a patient asked me about taking arginine , i would have no [ qualms ] with it , '' he says .  \n",
      "\n",
      "my ex screamed to me several times : `` if you ever leave me , i 'll take you for every penny you have and you 'll never see your son again ! '' family court made sure she took every penny i had and half of all future earnings until i die .  \n",
      "\n",
      "perhaps the power of brother who 'd made a resolute decision a day earlier based on what his mother would have wanted had changed his mind .  \n",
      "\n",
      "the sale could happen as soon as march if conditions were right .  \n",
      "\n",
      "on thursday , he said that if najib had done anything wrong he would `` face the consequences '' .  \n",
      "\n",
      "on monday , [ sarah huckabee sanders ] said he 'd given `` false testimony '' and accused him of `` leaking privileged information to journalists . '' when asked by a reporter if she was saying that comey had perjured himself when testifying before congress , she said , `` i think that 's something , probably , for [ the justice department ] to look at , not me .  \n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------- \n",
      "\n",
      "False Negatives, i.e., wrongly predicted as non-counterfactuals\n",
      "\n",
      "he could , for instance , have urged the house to pass the senate health bill , which is imperfect but better than nothing ; or he could have reached out to republicans by offering compromises .  \n",
      "\n",
      "assuming profit margins were high , then more capital ought to be ploughed into businesses until investment led competition drives margins back down .  \n",
      "\n",
      "he would n't have taken the job if he did n't write `` the obama doctrine . '' `` i should n't say this , because i 'm working on a book about the middle east , but the middle east   and foreign policy   can wear you down .  \n",
      "\n",
      "`` psa 's attack on jobs concerns all the group 's sites . '' government test unveiling the cutbacks on july 12 , varin had said that any further delay `` would have put the group in great danger '' .  \n",
      "\n",
      "yeah , and there would be no sti 's if every person on the planet had sex with no more than one person in their life .  \n",
      "\n",
      "he acknowledged that more testing could have helped address the problems but dismissed calls from swiss politicians for compensation .  \n",
      "\n",
      "`` if i could wave a magic wand and change ( the start ) from 2014 to 2015 , i would , '' said sandy praeger , kansas ' elected insurance commissioner , whose plan for a state partnership exchange was rejected by gov . sam brownback .  \n",
      "\n",
      "if josh fox and gasland were correct in their claims that the oil and gas industry is exempt from the clean water act , then this action announced today by epa could never have happened because epa would not have had the authority to carry it out .  \n",
      "\n",
      "many health policy experts say the penalties would be more effective if they were tougher .  \n",
      "\n",
      "that argument might make sense if the company had capacity issues   and it has admitted in the past that it does n't   or if had n't previously offered unlimited data .  \n",
      "\n",
      "in a paper published on thursday , a week after the 47th anniversary of the first moon landing , dr. delp and a nasa affiliated team of researchers examined how deep space travel may have affected the cardiovascular health of apollo astronauts , and supposing that astronauts set sail for distant galaxies right now , this research could save their lives .  \n",
      "\n",
      "david broder : if only democrats would embrace republicans like clinton embraced gingrich , the world would be a better place .  \n",
      "\n",
      "if jpmorgan chase was `` forced '' or even just strongly guided into making the acquisitions for which it is being penalized   and if it would not have undertaken them otherwise   then dimon is right in asserting that it 's `` unfair '' to face penalties .  \n",
      "\n",
      "`` if a romney obama matchup were held today , registered voters would divide 51 percent for the president to 44 percent for the former massachusetts governor , '' the washington post reported on tuesday .  \n",
      "\n",
      "i wish my adult child did n't suffer the first half of her life but we also did n't create it and it was intense parenting that kept her alive and thriving when she wanted to die .  \n",
      "\n",
      "it expects such measures would have boosted fixed income 's return on equity over the past three years   a key measure of profitability   to 16 percent if calculated under basel iii from the paltry 7 percent it would be now under these rules .  \n",
      "\n",
      "in all , patients taking the drug lost only between 3 % and 3.7 % of their body weight beyond what they would have with a placebo , the study showed .  \n",
      "\n",
      "the fda was critical of the researchers , led by james wilson , md , director of the institute for human gene therapy at the university of pennsylvania , saying that they should n't have given gelsinger the therapy because his ammonia levels were too high immediately prior to the treatment .  \n",
      "\n",
      "`` can you imagine if i said the things she said ? '' mr. trump told the crowd .  \n",
      "\n",
      "chris says : `` perhaps when people like teachers were first being priced out of london , we should have realised that a decade or so later these forces would affect us too . '' but he questions whether the 1980s and 1990s , when his cohort had it so good , really were a golden past whose norms can be recaptured .  \n",
      "\n",
      "if mr. biden were to have charged a similar range of fees for all his comparable speeches since leaving office , he would most likely have collected between $ 4 million and $ 5 million through speeches over the last two years  \n",
      "\n",
      "if elections were held today , polls show that the coalition would fall short of a majority in parliament , giving mr strache a chance to become chancellor .  \n",
      "\n",
      "even if the late saver continued putting away that same amount as scheduled until age 30 , they 'd still come up short unless they found a way for money to multiply on its own .  \n",
      "\n",
      "if shire 's 60 per cent jump in third quarter profits would come days after the collapse of its proposed £32bn merger with abbvie , an actual impossible event , the fears that the company might have been distracted during the turbulent past few months would have been confounding .  \n",
      "\n",
      "`` if it were up to it managers , blackberry would still be the device of choice , but with employees bringing their own devices there is no going back , '' said dan croft , ceo of mission critical wireless .  \n",
      "\n",
      "even if the juul craze carries on , it would only become a serious hazard if vapers progressed to smoking as a consequence of obtained a nicotine addiction from the juul .  \n",
      "\n",
      "rajaratnam 's lawyers had argued the recordings should have been suppressed because the initial wiretap application contained misstatements or omissions .  \n",
      "\n",
      "if david brooks were right in saying that americans do n't long for their leaders to be saints , my book and its distinctive test would be unnecessary .  \n",
      "\n",
      "although markets were spooked by a rise in hourly earnings , perhaps they should not have been .  \n",
      "\n",
      "to take a more relevant question that is key to mr coates 's article , one might ask whether african americans would be better off if federal housing policy had not discriminated against them from the 1930s through at least the 1960s .  \n",
      "\n",
      "even if the 538 electors were somehow men and women of profound virtue and valour , blessed with a deep understanding of what america needs in a president , it would still be antithetical to democratic principles to untether their vote from the results of the actual vote on election day .  \n",
      "\n",
      "mnsure officials might have known about many of these problems if they had tested the site with consumers prior to oct .  \n",
      "\n",
      "happy monday and welcome to morning energy , where in hindsight your host realizes that it may have been better for his body if he would have actually trained for yesterday 's cherry blossom 10 miler .  \n",
      "\n",
      "has already ousted ghosn as chairman , and ceo hiroto saikawa has told french media that renault would reach the same conclusion if it had access to all relevant information .  \n",
      "\n",
      "beyond this basic common sense there 's the fact this criticism and rationalizing might have more weight if it was n't coming from a group of lying hypocrites .  \n",
      "\n",
      "and if there were such a list as `` great dust mites of science '' , then circassia 's would surely top it .  \n",
      "\n",
      "early research has even suggested that the placebo effect ' could have therapeutic results on a whopping 35 % of patients , but a new study casts doubt on the placebo 's power to act as though it is a miracle cure .  \n",
      "\n",
      "without passive investing , portfolio managers could have allowed themselves to be more pragmatic in their investment process by rewarding assets in economies under positive transformation and shying away from investment in imbalanced economies ( which may have substantially higher weights in global em indices ) .  \n",
      "\n",
      "ferc said its investigators found the bank 's houston based traders engaged in 12 `` manipulative bidding strategies designed to make profits from power plants . '' the plants built in the 1950s and 1960s were less efficient than modern units and without the bidding strategies would not have operated very often , potentially costing the bank millions .  \n",
      "\n",
      "new research using biological measures of energy expenditure confirm something that with common sense should have already been obvious : regular , moderate exercise is healthier than isolated bouts of intense , exercise followed by a return to couch potato lifestyle , according to klaas r .  \n",
      "\n",
      "still , the companies believed they could have bridged the differences if germany was more willing to negotiate .  \n",
      "\n",
      "for example , pinal county in arizona might have no insurer selling marketplace policies for 2017 unless another company comes in and wipes any trace of aetna from existence and completely replaces it .  \n",
      "\n",
      "stephen kaye , the director of the community living policy center at the university of california , san francisco , recently examined how much less might have been budgeted for those services and others for people with physical and other nondevelopmental disabilities if the house bill 's caps had been in existence from 2001 to 2013 .  \n",
      "\n",
      "mcauliffe also took issue with northam 's delayed denial of being in the picture , saying on cnn sunday the governor should have `` come out immediately '' with an explanation .  \n",
      "\n",
      "there 's no record of her registering to be a donor before she died , yet assuming that she decided to do so while she was alive , someone being saved as a result was a real possibility .  \n",
      "\n",
      "he erred , first , in allowing semi pro agitators to lure him into an exchange about corporations when he should have had the presence of mind to calmly clarify that to increase or eliminate the cap on the payroll tax is just what he said it is : to raise taxes on people .  \n",
      "\n",
      "walter white , protagonist of the us tv drama , would receive free treatment for cancer if he was from st albans , not albuquerque .  \n",
      "\n",
      "`` this would have provided a more powerful remedy for a successful plaintiff compared to a mere monetary judgment , '' sleeth notes .  \n",
      "\n",
      "so not that he should n't have made a decision about whether the president obstructed justice or not in a criminal sense , but that concurrent with that , he should have put out more of their evidence or analysis to fill out the public picture .  \n",
      "\n",
      "`` if i was the government i would want to issue it , '' says karyn cavanaugh , a market strategist at voya .  \n",
      "\n",
      "we 've told them their lack of a state is their own fault ; if only they would embrace non violence , a reasonable and unprejudiced world would see the merit of their claims .  \n",
      "\n",
      "one year later , on what would have been their son 's first birthday if not for his premature death , the couple founded a non profit they called charlie 's kids foundation to raise awareness of pediatric sleep related deaths .  \n",
      "\n",
      "if the value of such services had been overstated in america , then the growth of real value added in finance would need to be reduced , but that of non financial industries would be increased , with no effect on overall gdp growth .  \n",
      "\n",
      "also , as an editorial accompanying the new study pointed out , if treating dcis . was preventing invasive cancers , then the incidence of those cancers should have dropped now that 60,000 cases of dcis . are being found and treated each year .  \n",
      "\n",
      "another argument is that wrongful birth lawsuits insult disabled persons by telling them that they would have been aborted had their parents known of their impairments .  \n",
      "\n",
      "the agency started to warn about the gloves in 1997 but refrained from banning them then , largely because it determined that pulling them from the market at the time could have caused shortages and been disruptive to the practice of medicine .  \n",
      "\n",
      "if we had grown up in environments where anger expression is forbidden , we would learn to deny our feelings and repress our memories .  \n",
      "\n",
      "would trump benefit if russia had greater control of areas in ukraine or the balkans ? we do n't know .  \n",
      "\n",
      "treasury secretary timothy geithner previewed tomorrow 's announcement for time managing editor richard stengel at the time warner economic summit in new york : `` there 's a bunch of things that would be nice to do if we had time and infinite capital , but we 're trying to focus on the practical issues that were at the core of this problem ,   \n",
      "\n",
      "the sad part about all this , sasich says , is that some of these studies could have provided important safety information to boost the fda 's voluntary reporting system for adverse reactions from drugs presuming they had been more careful .  \n",
      "\n",
      "`` if you 'd said to people back in 1948 spending will need to rise by 4 per cent a year , that 's more than inflation , significantly outpacing gdp ' , people would have said : this is utterly unrealistic and unachievable ' , '' she said .  \n",
      "\n",
      "that simplicity has helped buy those poor farmers ' support , support so intense that mr thaksin might still win an election in thailand today if he were not living in exile and subject to an arrest warrant .  \n",
      "\n",
      "if we returned to the world that existed before september 2007 , our balance sheet today , given currency growth over the last two years , would probably be about $ 1 trillion .  \n",
      "\n",
      "`` but they would not get serious about cleaning up their platforms , provided that the majority of consumers had not demanded it , '' she told the graduates .  \n",
      "\n",
      "but you could have done betterof course , you 'd have to go further back in time   when these companies were smaller .  \n",
      "\n",
      "`` i would be concerned if i were a producer relying on that market , given the upcoming circumstances that will affect it , '' said exporter greg corra , who runs inland trading co from his farm outside canberra .  \n",
      "\n",
      "if concerned outsiders are to help while discouraging talk of armed intervention , they should have pressed for a promise for a peaceful transition , likely involving negotiations on political and economic reform between the government and opposition and some form of transitional administration .  \n",
      "\n",
      "those expenses would have otherwise fallen in 2018 , he said .  \n",
      "\n",
      "in the worst cases , the regulator found customers were waiting `` hundreds of working days '' for switches that could have been conducted in a few days or weeks .  \n",
      "\n",
      "but the president is closing strong , '' by edward isaac dovere and carrie budoff brown : `` if president obama 's year ended in november , it would have been one of the worst of his presidency .  \n",
      "\n",
      "`` he would not have bought in if he did n't think our strategy good , '' nikolaus von bomhard told a news conference , referring to buffett 's voting stake in the world 's biggest reinsurer , last reported to be at more than 5 percent .  \n",
      "\n",
      "and if markos can put pictures of his adorable infant and toddler on the site , i 'm hoping i can get away with calling out my beloved kids by name matthew , faith , jackson and micaela and saying : you four have made my entire life so much more than it would have been without you .  \n",
      "\n",
      "`` `` it 's a very strong sun out there , so its better if you can dig and move around underground like a mole to avoid the heat , '' she said . ''  \n",
      "\n",
      "hackett and farley clearly think that ford could be a bigger player in india if it had lower cost products that it could build and sell profitably .  \n",
      "\n",
      "but this would suggest that gays should be forced to marry each other , if not for the fact that the bible also thinks people who have gay sex should be killed , or will go to hell .  \n",
      "\n",
      "monaco , md , who had his voice box removed due to cancer seven years ago , says he would consider having a transplant if he were younger .  \n",
      "\n",
      "the hope is that off the shelf therapies initially for blood cancers could have sped up treatment with an earlier implementation and cut costs because more patients benefit if the high cost of it fell  \n",
      "\n",
      "he did n't have to acquiesce to the emergency declaration   he could have asserted the senate 's independence at a critical time by passing the spending bill without validating mr. trump 's emergency declaration .  \n",
      "\n",
      "even if we had financing , chad would be either in the g5 or minusma .  \n",
      "\n",
      "i think marijuana has a lack of evidence behind some things for benefit and may have some downsides , but i do not think marijuana , per se , is a highly risky therapy , but on the otherhand if i thought it was , i could never recommend it .  \n",
      "\n",
      "readers would have scoffed if this column had forecast , back in 2006 , that short rates would be cut to zero and below ; that trillions of dollars of government bonds would trade on negative yields ; and that even the ultra cautious european central bank would join its peers in wholesale purchases of government debt .  \n",
      "\n",
      "but when i read about the resurgence of vaccine preventable diseases like measles , polio or pertussis , i worry   and wish that there was n't so much fear about vaccines .  \n",
      "\n",
      "if they just had the opportunity to rip the reins away from the liberals ( and all the actors who had not provided comic relief to a chimp ) , they could set things right ( tm ) .  \n",
      "\n",
      "palliative care , which focuses on relieving the discomfort and distress of serious illness , might have helped since it was unfortunately never able to be used before it was too late .  \n",
      "\n",
      "many have noted that were kavanaugh not a white man from a privileged background , his lying under oath , the accumulating accusations , and his belligerent behavior toward senators on the judiciary committee easily would have scuttled his nomination .  \n",
      "\n",
      "neither is in place right now , i would turn more bearish on global equities if they were .  \n",
      "\n",
      "if investors could forecast future economic growth , then goldman would be right : superior returns would be achieved .  \n",
      "\n",
      "`` japanese companies can be more globally competitive if they could have created a platform where data is not only collected but solutions can also be provided , as this would , on the whole , prove more attractive to customers '' says takushi ishikura , chief consultant at mitsubishi ufj research and consulting .  \n",
      "\n",
      "it 's exciting , because it appears that we can deliver toxic agents where we want them and nowhere else , and this concept may have applicability to other drugs other than chemotherapeutic agents , but in different circumstances where it did not appear so , it would be a major disappointment .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"False Positives, i.e., wrongly predicted as counterfactuals\\n\")\n",
    "# print('$$$$$$$$$$$$$$')\n",
    "for x in false_pos:\n",
    "    print(x,'\\n')\n",
    "print('\\n','-'*89,'\\n')\n",
    "print(\"False Negatives, i.e., wrongly predicted as non-counterfactuals\\n\")\n",
    "for x in false_neg:\n",
    "    print(x,'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
